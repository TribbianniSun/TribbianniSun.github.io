<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tribbiannisun.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://tribbiannisun.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-10T06:28:11+00:00</updated><id>https://tribbiannisun.github.io//feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">从“心静自然凉”到“空调开得大自然凉”</title><link href="https://tribbiannisun.github.io//blog/2024/%E5%BF%83%E9%9D%99%E8%87%AA%E7%84%B6%E5%87%89/" rel="alternate" type="text/html" title="从“心静自然凉”到“空调开得大自然凉”"/><published>2024-07-02T12:14:32+00:00</published><updated>2024-07-02T12:14:32+00:00</updated><id>https://tribbiannisun.github.io//blog/2024/%E5%BF%83%E9%9D%99%E8%87%AA%E7%84%B6%E5%87%89</id><content type="html" xml:base="https://tribbiannisun.github.io//blog/2024/%E5%BF%83%E9%9D%99%E8%87%AA%E7%84%B6%E5%87%89/"><![CDATA[<p>湾区的夏天是极为炎热的，七月初这一周的平均最高温度是达到了30摄氏度以上。尤其今天，报道中的最高温度更是达到了37度，在阳光的加持下，体感温度是妥妥的超过40摄氏度了。</p> <p>即使是从车门里出来以后走到办公楼里的那几步路，也显得极为艰难。直观感觉十分难受，浑身上下发黏，大太阳晒到脸上也是滚烫。这种酷暑难耐的感觉令我难免想起小时候夏天里发生的一件小事。</p> <p>我的家乡南京属于亚热带季风气候，四季分明，夏季极其炎热。当时我是小学二三年级左右，快放暑假的南京六月份天气应该是在35-40摄氏度。小学教室里设施比较老旧，没有空调，电扇的作用已经记不清了，所以就几乎等于没有吧。东南沿海地区湿热的感觉是走在街上走个十几分钟人都会满头大汗，南京那边说法叫小孩子“蒸笼头”。有那么一个中午午休的时候，我和一两个同学在走廊上闲逛的时候突然发现某个房间里往外冒着冷气，这冷气对于泡在酷暑里的孩子们来说肯定是一种巨大的诱惑。我们不约而同的在门口站了十分钟左右，顿感十分舒服。直到突然有一个不认识的老师把我们抓住，说这是校长室，不能站在校长室门口，这会扰乱秩序，剩余的中午午休时间我们都在没有空调的教室门口罚站。</p> <p>在拉我们去教室门口的时候，这位老师说出了一句影响我颇深的话：”心静自然凉！你们就是太浮躁了，所以感觉热！在门口站站让心静一静就凉快了。“当时罚站的那么十几分钟里我就思考这句话-”心静自然凉“。我以前没听过这样式的说法啊，仔细一想啊，心里浮躁的时候，自然感到燥热，心情平静，肯定会凉快啊！这老师有水平！</p> <p>初中以后教学楼里是装了空调的，夏天显得没那么热了，我也很少听到这句”心静自然凉“。下次见到这句话可能是高中学习唯物主义唯心主义的时候，这句话出现在过某个选项里，显然高中的哲学教育让我意识到这是一句很明显的唯心主义俗语，也明白了当时感到这句话很精妙的原因可能是：这可能是十岁不到的我接触的第一个唯心主义思想。</p> <p>思绪回到现实，在大太阳下走了几分钟以后，我从园区迈进了办公楼，26摄氏度的体温让我感到极为舒适，这种从难受到舒适的反差不禁让我想起了那句话-“心静自然凉”。我看不如改成“空调开得大自然凉”！十几年后再回想老师那句话，我也悟出了不同的意味。显然小学教室热是因为里面没装空调，温度40摄氏度是客观的，所以主观感到热是顺理成章的。老师这句话却有一种矛盾转移的嫌疑：把本来天气很热的客观事实，转移于人的内心不够平静。和小学同学讨论以后，我意识到有的时候环境温度不热，但身体发热，那是自己心态浮躁，而有的时候环境温度已经40度了，但强调是因为小朋友心态浮躁所以感到炎热，这也是一种错误的矛盾转移，明明是小学无法解决空调问题，却说成学生心态浮躁，这显然是荒谬的。这就是一种没有看清主要矛盾次要矛盾，可能温度占了90%，心里浮躁占了10%，但因为自己解决不了90%的主要矛盾，便故意突出次要矛盾，这显然是值得被质疑的。可我当时笨啊，看不出来啊。</p> <p>更严重的是，这句俗语在往后的十几年里给我的思维方式造成了很大的塑造：每每遇见夏天我感到炎热的时候，我总会先自我反省，是不是自己心太浮躁了所以导致感到热啊？作为举一反三，我也在遇到问题的时候常常考虑，是不是自己做的不好所以出现问题了啊。之前的思考方式是这样的：感到热，检查是不是自身浮躁，再检查环境温度。我目前认知的思考方式是这样的：感到热，先检查环境温度是不是真的过热，再检查我的心态是不是浮躁。从“心静自然凉”到“空调开得大自然凉”，我花了十几年。</p> <p>空调开的大确实是凉爽的。</p>]]></content><author><name></name></author><category term="thoughts"/><category term="random-thoughts"/><summary type="html"><![CDATA[一篇随笔]]></summary></entry><entry><title type="html">conda virtual environment</title><link href="https://tribbiannisun.github.io//blog/2024/conda-virtual-env/" rel="alternate" type="text/html" title="conda virtual environment"/><published>2024-07-02T12:14:32+00:00</published><updated>2024-07-02T12:14:32+00:00</updated><id>https://tribbiannisun.github.io//blog/2024/conda-virtual-env</id><content type="html" xml:base="https://tribbiannisun.github.io//blog/2024/conda-virtual-env/"><![CDATA[<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> venv new <span class="nv">python</span><span class="o">=</span>3.9 <span class="c"># craete the first virtual env</span>
conda activate venv <span class="c"># actiave the env</span>
pip list <span class="c"># check that we have a clear pip list</span>
pip <span class="nb">install </span>requests <span class="c"># install request package</span>
pip list <span class="c"># check that we installed the requests in the previous step</span>
pip freeze <span class="o">&gt;</span> requirements.txt <span class="c"># freeze the current requirements into this txt for other environments</span>
conda deactivate venv <span class="c"># deactivate the current virtual environment</span>
conda create <span class="nt">-n</span> venv-child new <span class="nv">python</span><span class="o">=</span>3.9 <span class="c"># craete another new virtual env</span>
conda activate venv-child <span class="c"># actiave the new env</span>
pip list <span class="c"># check that we have a clear pip list</span>
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt <span class="c"># install the packages listed in requirements.txt</span>
pip list <span class="c"># check that we successfull installed the required packages</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="developing,"/><category term="research"/><category term="python-developing"/><summary type="html"><![CDATA[This is an article which shows how to create, manage, and freeze virtual environment using conda.]]></summary></entry><entry><title type="html">GNN Research - Collecting papers on defense on graph neural network [26 - 50]</title><link href="https://tribbiannisun.github.io//blog/2024/GNN-research-2/" rel="alternate" type="text/html" title="GNN Research - Collecting papers on defense on graph neural network [26 - 50]"/><published>2024-05-20T12:14:32+00:00</published><updated>2024-05-20T12:14:32+00:00</updated><id>https://tribbiannisun.github.io//blog/2024/GNN-research-2</id><content type="html" xml:base="https://tribbiannisun.github.io//blog/2024/GNN-research-2/"><![CDATA[<h2 id="26---group-property-inference-attacks-against-graph-neural-networks"><a href="https://arxiv.org/abs/2209.01100">26 - Group Property Inference Attacks Against Graph Neural Networks</a></h2> <h3 id="abstract">Abstract</h3> <p>With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet. In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA’s success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.</p> <h3 id="code">Code</h3> <h2 id="27---defending-graph-convolutional-networks-against-dynamic-graph-perturbations-via-bayesian-self-supervision"><a href="https://arxiv.org/abs/2203.03762">27 - Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-supervision</a></h2> <h3 id="abstract-1">Abstract</h3> <p>In recent years, plentiful evidence illustrates that Graph Convolutional Networks (GCNs) achieve extraordinary accomplishments on the node classification task. However, GCNs may be vulnerable to adversarial attacks on label-scarce dynamic graphs. Many existing works aim to strengthen the robustness of GCNs; for instance, adversarial training is used to shield GCNs against malicious perturbations. However, these works fail on dynamic graphs for which label scarcity is a pressing issue. To overcome label scarcity, self-training attempts to iteratively assign pseudo-labels to highly confident unlabeled nodes but such attempts may suffer serious degradation under dynamic graph perturbations. In this paper, we generalize noisy supervision as a kind of self-supervised learning method and then propose a novel Bayesian self-supervision model, namely GraphSS, to address the issue. Extensive experiments demonstrate that GraphSS can not only affirmatively alert the perturbations on dynamic graphs but also effectively recover the prediction of a node classifier when the graph is under such perturbations. These two advantages prove to be generalized over three classic GCNs across five public graph datasets.</p> <h3 id="code-1">Code</h3> <ul> <li><a href="https://github.com/junzhuang-code/GraphSS?utm_source=catalyzex.com">github</a></li> </ul> <h2 id="28---adversarial-training-for-graph-neural-networks-pitfalls-solutions-and-new-directions"><a href="https://arxiv.org/abs/2203.03762">28 - Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions</a></h2> <h3 id="abstract-2">Abstract</h3> <p>Despite its success in the image domain, adversarial training did not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.</p> <h3 id="code-2">Code</h3> <ul> <li><a href="https://www.cs.cit.tum.de/daml/adversarial-training/">github</a></li> </ul> <h2 id="29---watermarking-graph-neural-networks-based-on-backdoor-attacks"><a href="https://arxiv.org/abs/2110.11024">29 - Watermarking Graph Neural Networks based on Backdoor Attacks</a></h2> <h3 id="abstract-3">Abstract</h3> <p>Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. Building a powerful GNN model is not a trivial task, as it requires a large amount of training data, powerful computing resources, and human expertise in fine-tuning the model. Moreover, with the development of adversarial attacks, e.g., model stealing attacks, GNNs raise challenges to model authentication. To avoid copyright infringement on GNNs, verifying the ownership of the GNN models is necessary. This paper presents a watermarking framework for GNNs for both graph and node classification tasks. We 1) design two strategies to generate watermarked data for the graph classification task and one for the node classification task, 2) embed the watermark into the host model through training to obtain the watermarked GNN model, and 3) verify the ownership of the suspicious model in a black-box setting. The experiments show that our framework can verify the ownership of GNN models with a very high probability (up to 99%) for both tasks. Finally, we experimentally show that our watermarking approach is robust against a state-of-the-art model extraction technique and four state-of-the-art defenses against backdoor attacks.</p> <h3 id="code-3">Code</h3> <ul> <li><a href="https://github.com/junzhuang-code/GraphSS?utm_source=catalyzex.com">github</a></li> </ul> <h2 id="30---adversarial-embedding-a-robust-and-elusive-steganography-and-watermarking-technique"><a href="https://arxiv.org/abs/1912.01487">30 - Adversarial Embedding: A robust and elusive Steganography and Watermarking technique</a></h2> <h3 id="abstract-4">Abstract</h3> <p>We propose adversarial embedding, a new steganography and watermarking technique that embeds secret information within images. The key idea of our method is to use deep neural networks for image classification and adversarial attacks to embed secret information within images. Thus, we use the attacks to embed an encoding of the message within images and the related deep neural network outputs to extract it. The key properties of adversarial attacks (invisible perturbations, nontransferability, resilience to tampering) offer guarantees regarding the confidentiality and the integrity of the hidden messages. We empirically evaluate adversarial embedding using more than 100 models and 1,000 messages. Our results confirm that our embedding passes unnoticed by both humans and steganalysis methods, while at the same time impedes illicit retrieval of the message (less than 13% recovery rate when the interceptor has some knowledge about our model), and is resilient to soft and (to some extent) aggressive image tampering (up to 100% recovery rate under jpeg compression). We further develop our method by proposing a new type of adversarial attack which improves the embedding density (amount of hidden information) of our method to up to 10 bits per pixel.</p> <h3 id="code-4">Code</h3> <ul> <li><a href="https://github.com/yamizi/Adversarial-Embedding">github</a></li> </ul> <h2 id="31---watermarking-neural-networks-with-watermarked-images"><a href="https://ieeexplore.ieee.org/document/9222304">31 - Watermarking Neural Networks With Watermarked Images</a></h2> <h3 id="abstract-5">Abstract</h3> <p>Watermarking neural networks is a quite important means to protect the intellectual property (IP) of neural networks. In this paper, we introduce a novel digital watermarking framework suitable for deep neural networks that output images as the results, in which any image outputted from a watermarked neural network must contain a certain watermark. Here, the host neural network to be protected and a watermark-extraction network are trained together, so that, by optimizing a combined loss function, the trained neural network can accomplish the original task while embedding a watermark into the outputted images. This work is totally different from previous schemes carrying a watermark by network weights or classification labels of the trigger set. By detecting watermarks in the outputted images, this technique can be adopted to identify the ownership of the host network and find whether an image is generated from a certain neural network or not. We demonstrate that this technique is effective and robust on a variety of image processing tasks, including image colorization, super-resolution, image editing, semantic segmentation and so on.</p> <h3 id="code-5">Code</h3> <ul> <li><a href="">github</a></li> </ul> <h2 id="32---deepsigns-an-end-to-end-watermarking-framework-for-ownership-protection-of-deep-neural-networks"><a href="https://www.researchgate.net/publication/332213758_DeepSigns_An_End-to-End_Watermarking_Framework_for_Ownership_Protection_of_Deep_Neural_Networks">32 - DeepSigns: An End-to-End Watermarking Framework for Ownership Protection of Deep Neural Networks</a></h2> <h3 id="abstract-6">Abstract</h3> <p>Deep Learning (DL) models have created a paradigm shift in our ability to comprehend raw data in various important fields, ranging from intelligence warfare and healthcare to autonomous transportation and automated manufacturing. A practical concern, in the rush to adopt DL models as a service, is protecting the models against Intellectual Property (IP) infringement. DL models are commonly built by allocating substantial computational resources that process vast amounts of proprietary training data. The resulting models are therefore considered to be an IP of the model builder and need to be protected to preserve the owner’s competitive advantage. We propose DeepSigns, the first end-to-end IP protection framework that enables developers to systematically insert digital watermarks in the target DL model before distributing the model. DeepSigns is encapsulated as a high-level wrapper that can be leveraged within common deep learning frameworks including TensorFlow and PyTorch. The libraries in DeepSigns work by dynamically learning the Probability Density Function (pdf) of activation maps obtained in different layers of a DL model. DeepSigns uses the low probabilistic regions within the model to gradually embed the owner’s signature (watermark) during DL training while minimally affecting the overall accuracy and training overhead. DeepSigns can demonstrably withstand various removal and transformation attacks, including model pruning, model fine-tuning, and watermark overwriting. We evaluate DeepSigns performance on a wide variety of DL architectures including wide residual convolution neural networks, multi-layer perceptrons, and long short-term memory models. Our extensive evaluations corroborate DeepSigns’ effectiveness and applicability. We further provide a highly-optimized accompanying API to facilitate training watermarked neural networks with a training overhead as low as 2.2%.</p> <h3 id="code-6">Code</h3> <ul> <li><a href="">github</a></li> </ul> <h2 id="33---robust-graph-data-learning-via-latent-graph-convolutional-representation"><a href="https://arxiv.org/abs/1904.11883">33 - Robust Graph Data Learning via Latent Graph Convolutional Representation</a></h2> <h3 id="abstract-7">Abstract</h3> <p>Graph Convolutional Representation (GCR) has achieved impressive performance for graph data representation. However, existing GCR is generally defined on the input fixed graph which may restrict the representation capacity and also be vulnerable to the structural attacks and noises. To address this issue, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is derived based on reformulating graph convolutional representation from the aspect of graph neighborhood reconstruction. Given an input graph A, LatGCR aims to generate a flexible latent graph A˜ for graph convolutional representation which obviously enhances the representation capacity and also performs robustly w.r.t graph structural attacks and noises. Moreover, LatGCR is implemented in a self-supervised manner and thus provides a basic block for both supervised and unsupervised graph learning tasks. Experiments on several datasets demonstrate the effectiveness and robustness of LatGCR.</p> <h3 id="code-7">Code</h3> <ul> <li><a href="">github</a></li> </ul> <h2 id="34---topology-attack-and-defense-for-graph-neural-networks-an-optimization-perspective"><a href="https://arxiv.org/abs/1906.04214">34 - Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective</a></h2> <h3 id="abstract-8">Abstract</h3> <p>Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph.</p> <h3 id="code-8">Code</h3> <ul> <li><a href="https://github.com/KaidiXu/GCN_ADV_Train">github</a></li> </ul> <h2 id="35---investigating-robustness-and-interpretability-of-link-prediction-via-adversarial-modifications"><a href="https://arxiv.org/abs/1905.00563">35 - Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications</a></h2> <h3 id="abstract-9">Abstract</h3> <p>Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.</p> <h3 id="code-9">Code</h3> <ul> <li><a href="https://github.com/pouyapez/criage">github</a></li> </ul> <h2 id="36---robust-graph-convolutional-networks-against-adversarial-attacks"><a href="http://pengcui.thumedialab.com/papers/RGCN.pdf">36 - Robust Graph Convolutional Networks Against Adversarial Attacks</a></h2> <h3 id="abstract-10">Abstract</h3> <p>Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-ofthe-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem. To address this problem, we propose Robust GCN (RGCN), a novel model that “fortifies” GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.</p> <h3 id="code-10">Code</h3> <ul> <li><a href="https://github.com/pouyapez/criage">github</a></li> </ul> <h2 id="37---virtual-adversarial-training-on-graph-convolutional-networks-in-node-classification"><a href="https://arxiv.org/abs/1902.11045">37 - Virtual Adversarial Training on Graph Convolutional Networks in Node Classification</a></h2> <h3 id="abstract-11">Abstract</h3> <h3 id="code-11">Code</h3> <ul> <li><a href="https://github.com/thumanlab/nrlweb/blob/master/static/assets/download/RGCN.zip">github</a></li> </ul> <h2 id="38---latent-adversarial-training-of-graph-convolution-networks"><a href="https://graphreason.github.io/papers/35.pdf">38 - Latent Adversarial Training of Graph Convolution Networks</a></h2> <h3 id="abstract-12">Abstract</h3> <p>Despite the recent success of graph convolution networks (GCNs) in modeling graph structured data, its vulnerability to adversarial attacks have been revealed and attacks on both node feature and graph structure have been designed. Direct extension of adversarial sample based defense algorithms meets with immediate challenge because computing the adversarial network requires substantial cost. We propose addressing this issue by perturbing the latent representations in GCNs, which not only dispenses with adversarial network generation, but also attains improved robustness and accuracy by respecting the latent manifold of the data. Experimental results confirm the superior performance over strong baselines.</p> <h3 id="code-12">Code</h3> <ul> <li><a href="https://github.com/cshjin/LATGCN">github</a></li> </ul> <h2 id="39---batch-virtual-adversarial-training-for-graph-convolutional-networks"><a href="https://arxiv.org/abs/1902.09192">39 - Batch Virtual Adversarial Training for Graph Convolutional Networks</a></h2> <h3 id="abstract-13">Abstract</h3> <p>We present batch virtual adversarial training (BVAT), a novel regularization method for graph convolutional networks (GCNs). BVAT addresses the shortcoming of GCNs that do not consider the smoothness of the model’s output distribution against local perturbations around the input. We propose two algorithms, sample-based BVAT and optimization-based BVAT, which are suitable to promote the smoothness of the model for graph-structured data by either finding virtual adversarial perturbations for a subset of nodes far from each other or generating virtual adversarial perturbations for all nodes with an optimization process. Extensive experiments on three citation network datasets Cora, Citeseer and Pubmed and a knowledge graph dataset Nell validate the effectiveness of the proposed method, which establishes state-of-the-art results in the semi-supervised node classification tasks.</p> <h3 id="code-13">Code</h3> <ul> <li><a href="">github</a></li> </ul> <h2 id="40---adversarial-robustness-of-similarity-based-link-prediction"><a href="https://arxiv.org/abs/1909.01432">40 - Adversarial Robustness of Similarity-Based Link Prediction</a></h2> <h3 id="abstract-14">Abstract</h3> <p>Link prediction is one of the fundamental problems in social network analysis. A common set of techniques for link prediction rely on similarity metrics which use the topology of the observed subnetwork to quantify the likelihood of unobserved links. Recently, similarity metrics for link prediction have been shown to be vulnerable to attacks whereby observations about the network are adversarially modified to hide target links. We propose a novel approach for increasing robustness of similarity-based link prediction by endowing the analyst with a restricted set of reliable queries which accurately measure the existence of queried links. The analyst aims to robustly predict a collection of possible links by optimally allocating the reliable queries. We formalize the analyst problem as a Bayesian Stackelberg game in which they first choose the reliable queries, followed by an adversary who deletes a subset of links among the remaining (unreliable) queries by the analyst. The analyst in our model is uncertain about the particular target link the adversary attempts to hide, whereas the adversary has full information about the analyst and the network. Focusing on similarity metrics using only local information, we show that the problem is NP-Hard for both players, and devise two principled and efficient approaches for solving it approximately. Extensive experiments with real and synthetic networks demonstrate the effectiveness of our approach.</p> <h3 id="code-14">Code</h3> <ul> <li><a href="">github</a></li> </ul> <h2 id="41---adversarial-training-methods-for-network-embedding"><a href="https://arxiv.org/abs/1908.11514">41 - Adversarial Training Methods for Network Embedding</a></h2> <h3 id="abstract-15">Abstract</h3> <p>Network Embedding is the task of learning continuous node representations for networks, which has been shown effective in a variety of tasks such as link prediction and node classification. Most of existing works aim to preserve different network structures and properties in low-dimensional embedding vectors, while neglecting the existence of noisy information in many real-world networks and the overfitting issue in the embedding learning process. Most recently, generative adversarial networks (GANs) based regularization methods are exploited to regularize embedding learning process, which can encourage a global smoothness of embedding vectors. These methods have very complicated architecture and suffer from the well-recognized non-convergence problem of GANs. In this paper, we aim to introduce a more succinct and effective local regularization method, namely adversarial training, to network embedding so as to achieve model robustness and better generalization performance. Firstly, the adversarial training method is applied by defining adversarial perturbations in the embedding space with an adaptive L2 norm constraint that depends on the connectivity pattern of node pairs. Though effective as a regularizer, it suffers from the interpretability issue which may hinder its application in certain real-world scenarios. To improve this strategy, we further propose an interpretable adversarial training method by enforcing the reconstruction of the adversarial examples in the discrete graph domain. These two regularization methods can be applied to many existing embedding models, and we take DeepWalk as the base model for illustration in the paper. Empirical evaluations in both link prediction and node classification demonstrate the effectiveness of the proposed methods.</p> <h3 id="code-15">Code</h3> <ul> <li><a href="https://github.com/wonniu/AdvT4NE_WWW2019">github</a></li> </ul> <h2 id="42---graph-interpolating-activation-improves-both-natural-and-robust-accuracies-in-data-efficient-deep-learning"><a href="https://arxiv.org/abs/1907.06800">42 - Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning</a></h2> <h3 id="abstract-16">Abstract</h3> <p>Improving the accuracy and robustness of deep neural nets (DNNs) and adapting them to small training data are primary tasks in deep learning research. In this paper, we replace the output activation function of DNNs, typically the data-agnostic softmax function, with a graph Laplacian-based high dimensional interpolating function which, in the continuum limit, converges to the solution of a Laplace-Beltrami equation on a high dimensional manifold. Furthermore, we propose end-to-end training and testing algorithms for this new architecture. The proposed DNN with graph interpolating activation integrates the advantages of both deep learning and manifold learning. Compared to the conventional DNNs with the softmax function as output activation, the new framework demonstrates the following major advantages: First, it is better applicable to data-efficient learning in which we train high capacity DNNs without using a large number of training data. Second, it remarkably improves both natural accuracy on the clean images and robust accuracy on the adversarial images crafted by both white-box and black-box adversarial attacks. Third, it is a natural choice for semi-supervised learning. For reproducibility, the code is available at \url{this https URL}.</p> <h3 id="code-16">Code</h3> <ul> <li><a href="https://github.com/BaoWangMath/DNN-DataDependentActivation">github</a></li> </ul> <h2 id="43---bayesian-graph-convolutional-neural-networks-for-semi-supervised-classification"><a href="https://arxiv.org/abs/1811.11103">43 - Bayesian graph convolutional neural networks for semi-supervised classification</a></h2> <h3 id="abstract-17">Abstract</h3> <p>Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.</p> <h3 id="code-17">Code</h3> <ul> <li><a href="https://github.com/huawei-noah/BGCN">github</a></li> </ul> <h2 id="44---graph-adversarial-training-dynamically-regularizing-based-on-graph-structure"><a href="https://arxiv.org/abs/1902.08226">44 - Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</a></h2> <h3 id="abstract-18">Abstract</h3> <p>Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (\eg articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. Adversarial Training (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples. In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose Graph Adversarial Training (GraphAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GraphAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GraphAT, we employ it on a state-of-the-art graph neural network model — Graph Convolutional Network (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GraphAT which outperforms normal training on GCN by 4.51% in node classification accuracy. Codes are available via: this https URL.</p> <h3 id="code-18">Code</h3> <ul> <li><a href="https://github.com/fulifeng/GraphAT">github</a></li> </ul> <h2 id="45---graph-revised-convolutional-network"><a href="https://arxiv.org/abs/1911.07123">45 - Graph-Revised Convolutional Network</a></h2> <h3 id="abstract-19">Abstract</h3> <p>Graph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As real-world graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A theoretical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently outperforms strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse.</p> <h3 id="code-19">Code</h3> <ul> <li><a href="https://github.com/Maysir/GRCN">github</a></li> </ul> <h2 id="46---defensevgae-defending-against-adversarial-attacks-on-graph-data-via-a-variational-graph-autoencoder"><a href="https://arxiv.org/abs/2006.08900">46 - DefenseVGAE: Defending against Adversarial Attacks on Graph Data via a Variational Graph Autoencoder</a></h2> <h3 id="abstract-20">Abstract</h3> <p>Graph neural networks (GNNs) achieve remarkable performance for tasks on graph data. However, recent works show they are extremely vulnerable to adversarial structural perturbations, making their outcomes unreliable. In this paper, we propose DefenseVGAE, a novel framework leveraging variational graph autoencoders(VGAEs) to defend GNNs against such attacks. DefenseVGAE is trained to reconstruct graph structure. The reconstructed adjacency matrix can reduce the effects of adversarial perturbations and boost the performance of GCNs when facing adversarial attacks. Our experiments on a number of datasets show the effectiveness of the proposed method under various threat models. Under some settings it outperforms existing defense strategies. Our code has been made publicly available at this https URL.</p> <h3 id="code-20">Code</h3> <ul> <li><a href="https://github.com/zhangao520/defense-vgae">github</a></li> </ul> <h2 id="47---enhancing-graph-neural-network-based-fraud-detectors-against-camouflaged-fraudsters"><a href="https://arxiv.org/abs/2008.08692">47 - Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters</a></h2> <h3 id="abstract-21">Abstract</h3> <p>Graph Neural Networks (GNNs) have been widely applied to fraud detection problems in recent years, revealing the suspiciousness of nodes by aggregating their neighborhood information via different relations. However, few prior works have noticed the camouflage behavior of fraudsters, which could hamper the performance of GNN-based fraud detectors during the aggregation process. In this paper, we introduce two types of camouflages based on recent empirical studies, i.e., the feature camouflage and the relation camouflage. Existing GNNs have not addressed these two camouflages, which results in their poor performance in fraud detection problems. Alternatively, we propose a new model named CAmouflage-REsistant GNN (CARE-GNN), to enhance the GNN aggregation process with three unique modules against camouflages. Concretely, we first devise a label-aware similarity measure to find informative neighboring nodes. Then, we leverage reinforcement learning (RL) to find the optimal amounts of neighbors to be selected. Finally, the selected neighbors across different relations are aggregated together. Comprehensive experiments on two real-world fraud datasets demonstrate the effectiveness of the RL algorithm. The proposed CARE-GNN also outperforms state-of-the-art GNNs and GNN-based fraud detectors. We integrate all GNN-based fraud detectors as an opensource toolbox: this https URL. The CARE-GNN code and datasets are available at this https URL.</p> <h3 id="code-21">Code</h3> <ul> <li><a href="https://github.com/safe-graph/DGFraud">github</a></li> </ul> <h2 id="48---on-the-robustness-of-cascade-diffusion-under-node-attacks"><a href="https://www.cs.au.dk/~karras/robustIC.pdf">48 - On the Robustness of Cascade Diffusion under Node Attacks</a></h2> <h3 id="abstract-22">Abstract</h3> <p>How can we assess a network’s ability to maintain its functionality under attacks? Network robustness has been studied extensively in the case of deterministic networks. However, applications such as online information diffusion and the behavior of networked public raise a question of robustness in probabilistic networks. We propose three novel robustness measures for networks hosting a diffusion under the Independent Cascade (IC) model, susceptible to node attacks. The outcome of such a process depends on the selection of its initiators, or seeds, by the seeder, as well as on two factors outside the seeder’s discretion: the attack strategy and the probabilistic diffusion outcome. We consider three levels of seeder awareness regarding these two uncontrolled factors, and evaluate the network’s viability aggregated over all possible extents of node attacks. We introduce novel algorithms from building blocks found in previous works to evaluate the proposed measures. A thorough experimental study with synthetic and real, scale-free and homogeneous networks establishes that these algorithms are effective and efficient, while the proposed measures highlight differences among networks in terms of robustness and the surprise they furnish when attacked. Last, we devise a new measure of diffusion entropy that can inform the design of probabilistically robust networks.</p> <h3 id="code-22">Code</h3> <ul> <li><a href="https://github.com/allogn/robustness">github</a></li> </ul> <h2 id="49---on-the-stability-of-polynomial-spectral-graph-filters"><a href="https://ieeexplore.ieee.org/abstract/document/9054072">49 - On The Stability of Polynomial Spectral Graph Filters</a></h2> <h3 id="abstract-23">Abstract</h3> <p>Spectral graph filters are a key component in state-of-the-art machine learning models used for graph-based learning, such as graph neural networks. For certain tasks stability of the spectral graph filters is important for learning suitable representations. Understanding the type of structural perturbation to which spectral graph filters are robust lets us reason as to when we may expect them to be well suited to a learning task. In this work, we first prove that polynomial graph filters are stable with respect to the change in the normalised graph Laplacian matrix. We then show empirically that properties of a structural perturbation, specifically the relative locality of the edges removed in a binary graph, effect the change in the normalised graph Laplacian. Together, our results have implications on designing robust graph filters and representations under structural perturbation.</p> <h3 id="code-23">Code</h3> <ul> <li><a href="https://github.com/henrykenlay/spgf">github</a></li> </ul> <h2 id="50---graph-structure-learning-for-robust-graph-neural-networks"><a href="https://arxiv.org/abs/2005.10203">50 - Graph Structure Learning for Robust Graph Neural Networks</a></h2> <h3 id="abstract-24">Abstract</h3> <p>Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: this https URL). The specific experimental settings to reproduce our results can be found in this https URL.</p> <h3 id="code-24">Code</h3> <ul> <li><a href="https://github.com/DSE-MSU/DeepRobust">github-1</a></li> <li><a href="https://github.com/ChandlerBang/Pro-GNN">github-2</a></li> </ul> <p>some work credit to: https://github.com/EdisonLeeeee/Graph-Adversarial-Learning?tab=readme-ov-file</p>]]></content><author><name></name></author><category term="research"/><category term="gnn"/><summary type="html"><![CDATA[sharing 25 papers which propose novel techniques to achieve either attack or defense on graph neural networks.]]></summary></entry><entry><title type="html">GNN Research - Collecting papers on attack on graph neural network [1 - 25]</title><link href="https://tribbiannisun.github.io//blog/2024/GNN-research-1/" rel="alternate" type="text/html" title="GNN Research - Collecting papers on attack on graph neural network [1 - 25]"/><published>2024-05-19T12:14:32+00:00</published><updated>2024-05-19T12:14:32+00:00</updated><id>https://tribbiannisun.github.io//blog/2024/GNN-research-1</id><content type="html" xml:base="https://tribbiannisun.github.io//blog/2024/GNN-research-1/"><![CDATA[<h2 id="1---model-extraction-attacks-on-graph-neural-networks-taxonomy-and-realization"><a href="https://arxiv.org/abs/2010.12751">1 - Model extraction attacks on graph neural networks: Taxonomy and realization.</a></h2> <h3 id="abstract">Abstract</h3> <p>Machine learning models are shown to face a severe threat from Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by an attacker pretending as a client. Unfortunately, prior works focus on the models trained over the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, for the first time, we comprehensively investigate and develop model extraction attacks against GNN models. We first systematically formalise the threat modelling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbour connections of the nodes obtained by the attacker. Then we present detailed methods which utilise the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., 84% - 89% of the inputs in the target domain have the same output predictions as the victim model.</p> <h3 id="code">Code</h3> <ul> <li><a href="https://github.com/TrustworthyGNN/MEA-GNN">github</a></li> </ul> <h2 id="2---watermarking-graph-neural-networks-by-random-graphs"><a href="https://arxiv.org/abs/2011.00512">2 - Watermarking Graph Neural Networks by Random Graphs</a></h2> <h3 id="abstract-1">Abstract</h3> <p>Many learning tasks require us to deal with graph data which contains rich relational information among elements, leading increasing graph neural network (GNN) models to be deployed in industrial products for improving the quality of service. However, they also raise challenges to model authentication. It is necessary to protect the ownership of the GNN models, which motivates us to present a watermarking method to GNN models in this paper. In the proposed method, an Erdos-Renyi (ER) random graph with random node feature vectors and labels is randomly generated as a trigger to train the GNN to be protected together with the normal samples. During model training, the secret watermark is embedded into the label predictions of the ER graph nodes. During model verification, by activating a marked GNN with the trigger ER graph, the watermark can be reconstructed from the output to verify the ownership. Since the ER graph was randomly generated, by feeding it to a non-marked GNN, the label predictions of the graph nodes are random, resulting in a low false alarm rate (of the proposed work). Experimental results have also shown that, the performance of a marked GNN on its original task will not be impaired. Moreover, it is robust against model compression and fine-tuning, which has shown the superiority and applicability.</p> <h3 id="code-1">Code</h3> <ul> <li>blank</li> </ul> <h2 id="3---stealing-links-from-graph-neural-networks"><a href="https://arxiv.org/abs/2005.02131">3 - Stealing Links from Graph Neural Networks</a></h2> <h3 id="abstract-2">Abstract</h3> <p>Graph data, such as chemical networks and social networks, may be deemed confidential/private because the data owner often spends lots of resources collecting the data or the data contains sensitive information, e.g., social relationships. Recently, neural networks were extended to graph data, which are known as graph neural networks (GNNs). Due to their superior performance, GNNs have many applications, such as healthcare analytics, recommender systems, and fraud detection. In this work, we propose the first attacks to steal a graph from the outputs of a GNN model that is trained on the graph. Specifically, given a black-box access to a GNN model, our attacks can infer whether there exists a link between any pair of nodes in the graph used to train the model. We call our attacks link stealing attacks. We propose a threat model to systematically characterize an adversary’s background knowledge along three dimensions which in total leads to a comprehensive taxonomy of 8 different link stealing attacks. We propose multiple novel methods to realize these 8 attacks. Extensive experiments on 8 real-world datasets show that our attacks are effective at stealing links, e.g., AUC (area under the ROC curve) is above 0.95 in multiple cases. Our results indicate that the outputs of a GNN model reveal rich information about the structure of the graph used to train the model.</p> <h3 id="code-2">Code</h3> <ul> <li><a href="https://github.com/xinleihe/link_stealing_attack">github</a></li> </ul> <h2 id="4---node-level-membership-inference-attacks-against-graph-neural-networks"><a href="https://arxiv.org/abs/2102.05429">4 - Node-Level Membership Inference Attacks Against Graph Neural Networks</a></h2> <h3 id="abstract-3">Abstract</h3> <p>Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary’s background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack’s success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.</p> <h3 id="code-3">Code</h3> <ul> <li>blank</li> </ul> <h2 id="5---model-stealing-attacks-against-inductive-graph-neural-networks"><a href="https://arxiv.org/abs/2112.08331">5 - Model Stealing Attacks Against Inductive Graph Neural Networks</a></h2> <h3 id="abstract-4">Abstract</h3> <p>Many real-world data come in the form of graphs. Graph neural networks (GNNs), a new family of machine learning (ML) models, have been proposed to fully leverage graph data to build powerful applications. In particular, the inductive GNNs, which can generalize to unseen data, become mainstream in this direction. Machine learning models have shown great potential in various tasks and have been deployed in many real-world scenarios. To train a good model, a large amount of data as well as computational resources are needed, leading to valuable intellectual property. Previous research has shown that ML models are prone to model stealing attacks, which aim to steal the functionality of the target models. However, most of them focus on the models trained with images and texts. On the other hand, little attention has been paid to models trained with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the first model stealing attacks against inductive GNNs. We systematically define the threat model and propose six attacks based on the adversary’s background knowledge and the responses of the target models. Our evaluation on six benchmark datasets shows that the proposed model stealing attacks against GNNs achieve promising performance.</p> <h3 id="code-4">Code</h3> <ul> <li><a href="https://github.com/xinleihe/gnnstealing">github</a></li> </ul> <h2 id="6---membership-inference-attack-on-graph-neural-networks"><a href="https://arxiv.org/abs/2110.02631">6 - Membership Inference Attack on Graph Neural Networks</a></h2> <h2 id="abstract-5">Abstract</h2> <p>Graph Neural Networks (GNNs), which generalize traditional deep neural networks on graph data, have achieved state-of-the-art performance on several graph analytical tasks. We focus on how trained GNN models could leak information about the \emph{member} nodes that they were trained on. We introduce two realistic settings for performing a membership inference (MI) attack on GNNs. While choosing the simplest possible attack model that utilizes the posteriors of the trained model (black-box access), we thoroughly analyze the properties of GNNs and the datasets which dictate the differences in their robustness towards MI attack. While in traditional machine learning models, overfitting is considered the main cause of such leakage, we show that in GNNs the additional structural information is the major contributing factor. We support our findings by extensive experiments on four representative GNN models. To prevent MI attacks on GNN, we propose two effective defenses that significantly decreases the attacker’s inference by up to 60% without degradation to the target model’s performance. Our code is available at this https URL.</p> <h2 id="code-5">Code</h2> <ul> <li><a href="https://github.com/iyempissy/rebMIGraph">github</a></li> </ul> <h2 id="7---quantifying-privacy-leakage-in-graph-embedding"><a href="https://arxiv.org/abs/2010.00906">7 - Quantifying Privacy Leakage in Graph Embedding</a></h2> <h2 id="abstract-6">Abstract</h2> <p>Graph embeddings have been proposed to map graph data to low dimensional space for downstream processing (e.g., node classification or link prediction). With the increasing collection of personal data, graph embeddings can be trained on private and sensitive data. For the first time, we quantify the privacy leakage in graph embeddings through three inference attacks targeting Graph Neural Networks. We propose a membership inference attack to infer whether a graph node corresponding to individual user’s data was member of the model’s training or not. We consider a blackbox setting where the adversary exploits the output prediction scores, and a whitebox setting where the adversary has also access to the released node embeddings. This attack provides an accuracy up to 28% (blackbox) 36% (whitebox) beyond random guess by exploiting the distinguishable footprint between train and test data records left by the graph embedding. We propose a Graph Reconstruction attack where the adversary aims to reconstruct the target graph given the corresponding graph embeddings. Here, the adversary can reconstruct the graph with more than 80% of accuracy and link inference between two nodes around 30% more confidence than a random guess. We then propose an attribute inference attack where the adversary aims to infer a sensitive attribute. We show that graph embeddings are strongly correlated to node attributes letting the adversary inferring sensitive information (e.g., gender or location).</p> <h2 id="code-6">Code</h2> <ul> <li><a href="https://github.com/vasishtduddu/GraphLeaks">github</a></li> </ul> <h2 id="8---inference-attacks-against-graph-neural-networks"><a href="https://arxiv.org/abs/2110.02631">8 - Inference Attacks Against Graph Neural Networks</a></h2> <h2 id="abstract-7">Abstract</h2> <p>Graph is an important data representation ubiquitously existing in the real world. However, analyzing the graph data is computationally difficult due to its non-Euclidean nature. Graph embedding is a powerful tool to solve the graph analytics problem by transforming the graph data into low-dimensional vectors. These vectors could also be shared with third parties to gain additional insights of what is behind the data. While sharing graph embedding is intriguing, the associated privacy risks are unexplored. In this paper, we systematically investigate the information leakage of the graph embedding by mounting three inference attacks. First, we can successfully infer basic graph properties, such as the number of nodes, the number of edges, and graph density, of the target graph with up to 0.89 accuracy. Second, given a subgraph of interest and the graph embedding, we can determine with high confidence that whether the subgraph is contained in the target graph. For instance, we achieve 0.98 attack AUC on the DD dataset. Third, we propose a novel graph reconstruction attack that can reconstruct a graph that has similar graph structural statistics to the target graph. We further propose an effective defense mechanism based on graph embedding perturbation to mitigate the inference attacks without noticeable performance degradation for graph classification tasks. Our code is available at this https URL.</p> <h2 id="code-7">Code</h2> <ul> <li><a href="https://github.com/Zhangzhk0819/GNN-Embedding-Leaks">github</a></li> </ul> <h2 id="9---model-inversion-attacks-against-graph-neural-networks"><a href="https://arxiv.org/abs/2209.07807">9 - Model Inversion Attacks Against Graph Neural Networks</a></h2> <h2 id="abstract-8">Abstract</h2> <p>Many data mining tasks rely on graphs to model relational structures among individuals (nodes). Since relational data are often sensitive, there is an urgent need to evaluate the privacy risks in graph data. One famous privacy attack against data analysis models is the model inversion attack, which aims to infer sensitive data in the training dataset and leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion attacks on non-grid domains such as graph leads to poor attack performance. This is mainly due to the failure to consider the unique properties of graphs. To bridge this gap, we conduct a systematic study on model inversion attacks against Graph Neural Networks (GNNs), one of the state-of-the-art graph analysis tools in this paper. Firstly, in the white-box setting where the attacker has full access to the target GNN model, we present GraphMI to infer the private training graph data. Specifically, in GraphMI, a projected gradient module is proposed to tackle the discreteness of graph edges and preserve the sparsity and smoothness of graph features; a graph auto-encoder module is used to efficiently exploit graph topology, node attributes, and target model parameters for edge inference; a random sampling module can finally sample discrete edges. Furthermore, in the hard-label black-box setting where the attacker can only query the GNN API and receive the classification results, we propose two methods based on gradient estimation and reinforcement learning (RL-GraphMI). Our experimental results show that such defenses are not sufficiently effective and call for more advanced defenses against privacy attacks.</p> <h2 id="code-8">Code</h2> <h2 id="10---graphmi-extracting-private-graph-data-from-graph-neural-networks"><a href="https://arxiv.org/abs/2106.02820">10 - GraphMI: Extracting Private Graph Data from Graph Neural Networks</a></h2> <h2 id="abstract-9">Abstract</h2> <p>As machine learning becomes more widely used for critical applications, the need to study its implications in privacy turns to be urgent. Given access to the target model and auxiliary information, the model inversion attack aims to infer sensitive features of the training dataset, which leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion techniques on non-grid domains such as graph achieves poor attack performance due to the difficulty to fully exploit the intrinsic properties of graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack (GraphMI), which aims to extract private graph data of the training graph by inverting GNN, one of the state-of-the-art graph analysis tools. Specifically, we firstly propose a projected gradient module to tackle the discreteness of graph edges while preserving the sparsity and smoothness of graph features. Then we design a graph auto-encoder module to efficiently exploit graph topology, node attributes, and target model parameters for edge inference. With the proposed methods, we study the connection between model inversion risk and edge influence and show that edges with greater influence are more likely to be recovered. Extensive experiments over several public datasets demonstrate the effectiveness of our method. We also show that differential privacy in its canonical form can hardly defend our attack while preserving decent utility.</p> <h2 id="code-9">Code</h2> <ul> <li><a href="https://github.com/zaixizhang/GraphMI">github</a></li> </ul> <h2 id="11---ml-doctor-holistic-risk-assessment-of-inference-attacks-against-machine-learning-models"><a href="https://arxiv.org/abs/2102.02551">11 - ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models</a></h2> <h2 id="abstract-10">Abstract</h2> <p>Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks – namely, membership inference, model inversion, attribute inference, and model stealing – and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack’s performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.</p> <h2 id="code-10">Code</h2> <ul> <li><a href="https://github.com/liuyugeng/ML-Doctor">github</a></li> </ul> <h2 id="12---memguard-defending-against-black-box-membership-inference-attacks-via-adversarial-examples"><a href="https://arxiv.org/abs/1909.10594">12 - MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples</a></h2> <h2 id="abstract-11">Abstract</h2> <p>In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier’s training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample’s confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier’s training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples. Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker’s classifier. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.</p> <h2 id="code-11">Code</h2> <ul> <li><a href="https://github.com/jinyuan-jia/memguard">github</a></li> </ul> <h2 id="13---adversarial-attacks-on-neural-networks-for-graph-data"><a href="https://arxiv.org/abs/1805.07984">13 - Adversarial Attacks on Neural Networks for Graph Data</a></h2> <h2 id="abstract-12">Abstract</h2> <p>Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node’s features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.</p> <h2 id="code-12">Code</h2> <ul> <li><a href="https://github.com/danielzuegner/nettack">github</a></li> </ul> <h2 id="14---adversarial-attack-on-graph-structured-data"><a href="https://arxiv.org/abs/1806.02371">14 - Adversarial Attack on Graph Structured Data</a></h2> <h2 id="abstract-13">Abstract</h2> <p>Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.</p> <h2 id="code-13">Code</h2> <ul> <li><a href="https://github.com/Hanjun-Dai/graph_adversarial_attack">github</a></li> </ul> <h2 id="15---adversarial-examples-on-graph-data-deep-insights-into-attack-and-defense"><a href="https://arxiv.org/abs/1903.01610">15 - Adversarial Examples on Graph Data: Deep Insights into Attack and Defense</a></h2> <h2 id="abstract-14">Abstract</h2> <p>Graph deep learning models, such as graph convolutional networks (GCN) achieve remarkable performance for tasks on graph data. Similar to other types of deep models, graph deep learning models often suffer from adversarial attacks. However, compared with non-graph data, the discrete features, graph connections and different definitions of imperceptible perturbations bring unique challenges and opportunities for the adversarial attacks and defenses for graph data. In this paper, we propose both attack and defense techniques. For attack, we show that the discreteness problem could easily be resolved by introducing integrated gradients which could accurately reflect the effect of perturbing certain features or edges while still benefiting from the parallel computations. For defense, we observe that the adversarially manipulated graph for the targeted attack differs from normal graphs statistically. Based on this observation, we propose a defense approach which inspects the graph and recovers the potential adversarial perturbations. Our experiments on a number of datasets show the effectiveness of the proposed methods.</p> <h2 id="code-14">Code</h2> <ul> <li><a href="https://github.com/stellargraph/stellargraph">github</a></li> </ul> <h2 id="16---topology-attack-and-defense-for-graph-neural-networks-an-optimization-perspective"><a href="https://arxiv.org/abs/1906.04214">16 - Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective</a></h2> <h2 id="abstract-15">Abstract</h2> <p>Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semi-supervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradient-based attack, we propose the first optimization-based adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph.</p> <h2 id="code-15">Code</h2> <ul> <li><a href="https://github.com/KaidiXu/GCN_ADV_Train">github</a></li> </ul> <h2 id="17---adversarial-attacks-on-node-embeddings-via-graph-poisoning"><a href="https://arxiv.org/abs/1809.01093">17 - Adversarial Attacks on Node Embeddings via Graph Poisoning</a></h2> <h2 id="abstract-16">Abstract</h2> <p>The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.</p> <h2 id="code-16">Code</h2> <ul> <li><a href="https://github.com/abojchevski/node_embedding_attack">github</a></li> </ul> <h2 id="18---adversarial-attacks-on-graph-neural-networks-via-meta-learning"><a href="https://arxiv.org/abs/1902.08412">18 - Adversarial Attacks on Graph Neural Networks via Meta Learning</a></h2> <h2 id="abstract-17">Abstract</h2> <p>Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.</p> <h2 id="code-17">Code</h2> <ul> <li><a href="https://github.com/danielzuegner/gnn-meta-attack">github</a></li> </ul> <h2 id="19---peernets-exploiting-peer-wisdom-against-adversarial-attacks"><a href="https://arxiv.org/abs/1806.00088">19 - PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks</a></h2> <h2 id="abstract-18">Abstract</h2> <p>Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy.</p> <h2 id="code-18">Code</h2> <ul> <li><a href="https://github.com/tantara/PeerNets-pytorch">github</a></li> </ul> <h2 id="20---certifiable-robustness-to-graph-perturbations"><a href="https://arxiv.org/abs/1910.14356">20 - Certifiable Robustness to Graph Perturbations</a></h2> <h2 id="abstract-19">Abstract</h2> <p>Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.</p> <h2 id="code-19">Code</h2> <ul> <li><a href="https://github.com/abojchevski/graph_cert">github</a></li> </ul> <h2 id="21---a-unified-framework-for-data-poisoning-attack-to-graph-based-semi-supervised-learning"><a href="https://arxiv.org/abs/1910.14147">21 - A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning</a></h2> <h2 id="abstract-20">Abstract</h2> <p>In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals, and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases — poisoning regression tasks under ℓ2-norm constraint and classification tasks under ℓ0-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50\% error).</p> <h2 id="code-20">Code</h2> <h2 id="22---gnnguard-defending-graph-neural-networks-against-adversarial-attacks"><a href="https://arxiv.org/abs/2006.08149">22 - GNNGuard: Defending Graph Neural Networks against Adversarial Attacks</a></h2> <h2 id="abstract-21">Abstract</h2> <p>Deep learning methods for graphs achieve remarkable performance across a variety of domains. However, recent findings indicate that small, unnoticeable perturbations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop GNNGuard, a general algorithm to defend against a variety of training-time attacks that perturb the discrete graph structure. GNNGuard can be straight-forwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack.GNNGuard learns how to best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges allow for robust propagation of neural messages in the underlying GNN. GNNGuard introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across five GNNs, three defense methods, and five datasets,including a challenging human disease graph, experiments show that GNNGuard outperforms existing defense approaches by 15.3% on average. Remarkably, GNNGuard can effectively restore state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks, and can defend against attacks on heterophily graphs.</p> <h2 id="code-21">Code</h2> <ul> <li><a href="https://github.com/mims-harvard/GNNGuard">github</a></li> </ul> <h2 id="23---robust-graph-convolutional-networks-against-adversarial-attacks"><a href="https://github.com/ZW-ZHANG/RobustGCN">23 - Robust Graph Convolutional Networks Against Adversarial Attacks</a></h2> <h2 id="abstract-22">Abstract</h2> <p>Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-ofthe-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem. To address this problem, we propose Robust GCN (RGCN), a novel model that “fortifies” GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.</p> <h2 id="code-22">Code</h2> <ul> <li><a href="https://github.com/ZW-ZHANG/RobustGCN">github</a></li> </ul> <h2 id="24---a-restricted-black-box-adversarial-framework-towards-attacking-graph-embedding-models"><a href="https://arxiv.org/abs/1908.01297">24 - A Restricted Black-Box Adversarial Framework Towards Attacking Graph Embedding Models</a></h2> <h2 id="abstract-23">Abstract</h2> <p>With the great success of graph embedding model on both academic and industry area, the robustness of graph embedding against adversarial attack inevitably becomes a central problem in graph learning domain. Regardless of the fruitful progress, most of the current works perform the attack in a white-box fashion: they need to access the model predictions and labels to construct their adversarial loss. However, the inaccessibility of model predictions in real systems makes the white-box attack impractical to real graph learning system. This paper promotes current frameworks in a more general and flexible sense – we demand to attack various kinds of graph embedding model with black-box driven. To this end, we begin by investigating the theoretical connections between graph signal processing and graph embedding models in a principled way and formulate the graph embedding model as a general graph signal process with corresponding graph filter. As such, a generalized adversarial attacker: GF-Attack is constructed by the graph filter and feature matrix. Instead of accessing any knowledge of the target classifiers used in graph embedding, GF-Attack performs the attack only on the graph filter in a black-box attack fashion. To validate the generalization of GF-Attack, we construct the attacker on four popular graph embedding models. Extensive experimental results validate the effectiveness of our attacker on several benchmark datasets. Particularly by using our attack, even small graph perturbations like one-edge flip is able to consistently make a strong attack in performance to different graph embedding models.</p> <h2 id="code-23">Code</h2> <ul> <li><a href="https://github.com/SwiftieH/GFAttack">github</a></li> </ul> <h2 id="25---all-you-need-is-low-rank-defending-against-adversarial-attacks-on-graphs"><a href="https://dl.acm.org/doi/10.1145/3336191.3371789">25 - All You Need Is Low (Rank): Defending Against Adversarial Attacks on Graphs</a></h2> <h2 id="abstract-24">Abstract</h2> <p>Recent studies have demonstrated that machine learning approaches like deep learning methods are easily fooled by adversarial attacks. Recently, a highly-influential study examined the impact of adversarial attacks on graph data and demonstrated that graph embedding techniques are also vulnerable to adversarial attacks. Fake users on social media and fake product reviews are examples of perturbations in graph data that are realistic counterparts of the adversarial models proposed. Graphs are widely used in a variety of domains and it is highly important to develop graph analysis techniques that are robust to adversarial attacks. One of the recent studies on generating adversarial attacks for graph data is Nettack. The Nettack model has shown to be very successful in deceiving the Graph Convolutional Network (GCN) model. Nettack is also transferable to other node classification approaches e.g. node embeddings. In this paper, we explore the properties of Nettack perturbations, in search for effective defenses against them. Our first finding is that Nettack demonstrates a very specific behavior in the spectrum of the graph: only high-rank (low-valued) singular components of the graph are affected. Following that insight, we show that a low-rank approximation of the graph, that uses only the top singular components for its reconstruction, can greatly reduce the effects of Nettack and boost the performance of GCN when facing adversarial attacks. Indicatively, on the CiteSeer dataset, our proposed defense mechanism is able to reduce the success rate of Nettack from 98% to 36%. Furthermore, we show that tensor-based node embeddings, which by default project the graph into a low-rank subspace, are robust against Nettack perturbations. Lastly, we propose LowBlow, a low-rank adversarial attack which is able to affect the classification performance of both GCN and tensor-based node embeddings and we show that the low-rank attack is noticeable and making it unnoticeable results in a high-rank attack.</p> <h2 id="code-24">Code</h2> ]]></content><author><name></name></author><category term="research"/><category term="gnn"/><summary type="html"><![CDATA[sharing 25 papers which propose novel techniques to achieve either attack or defense on graph neural networks.]]></summary></entry><entry><title type="html">Leetcode Weekly Contest 398</title><link href="https://tribbiannisun.github.io//blog/2024/leetcode-weekly-contest-398/" rel="alternate" type="text/html" title="Leetcode Weekly Contest 398"/><published>2024-05-18T22:16:32+00:00</published><updated>2024-05-18T22:16:32+00:00</updated><id>https://tribbiannisun.github.io//blog/2024/leetcode-weekly-contest-398</id><content type="html" xml:base="https://tribbiannisun.github.io//blog/2024/leetcode-weekly-contest-398/"><![CDATA[<p>This is a post about <a href="https://leetcode.com/contest/weekly-contest-398/">weekly contest 398</a>.</p> <h2 id="3151-special-array-i">3151. Special Array I</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">isArraySpecial</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">nums</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">nums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="n">nums</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div> <h2 id="3152-special-array-ii">3152. Special Array II</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">isArraySpecial</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">nums</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">queries</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">nums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="n">nums</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">p</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">p</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">]</span> <span class="ow">in</span> <span class="n">queries</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">==</span> <span class="n">p</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="n">ret</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ret</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ret</span>
</code></pre></div></div> <h2 id="3153-sum-of-digit-differences-of-all-pairs">3153. Sum of Digit Differences of All Pairs</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">sumDigitDifferences</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">nums</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">nums</span> <span class="o">=</span> <span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">nums</span><span class="p">]</span>
        <span class="n">l</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="p">[{}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                <span class="n">mp</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">nums</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]]</span> <span class="o">=</span> <span class="n">mp</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="nf">get</span><span class="p">(</span><span class="n">nums</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
            <span class="n">local_ret</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">mp</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
                <span class="n">cnt</span> <span class="o">=</span> <span class="n">mp</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
                <span class="n">local_ret</span> <span class="o">+=</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">cnt</span><span class="p">)</span> <span class="o">*</span> <span class="n">cnt</span>
            <span class="n">local_ret</span> <span class="o">=</span> <span class="n">local_ret</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">ret</span> <span class="o">+=</span> <span class="n">local_ret</span>

        <span class="k">return</span> <span class="n">ret</span>

</code></pre></div></div> <h2 id="3154-find-number-of-ways-to-reach-the-k-th-stair">3154. Find Number of Ways to Reach the K-th Stair</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">waysToReachStair</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">N</span> <span class="o">=</span> <span class="mi">35</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">):</span>
                <span class="n">t</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
                    <span class="n">t</span> <span class="o">+=</span> <span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
                <span class="n">t</span> <span class="o">-=</span> <span class="n">j</span>
                <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="n">k</span><span class="p">:</span>
                    <span class="nf">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
                    <span class="n">ret</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>


        <span class="k">return</span> <span class="n">ret</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="code"/><category term="leetcode"/><summary type="html"><![CDATA[sharing my solutions to leetcode weekly contest 398]]></summary></entry></feed>